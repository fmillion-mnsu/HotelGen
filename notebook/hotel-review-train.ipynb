{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54364532-f3ea-4a26-bced-f8e53a19452e",
   "metadata": {},
   "source": [
    "# Hotel Review Generator\n",
    "\n",
    "This notebook will train an SLM to function as a hotel review generator based on structured inputs.\n",
    "\n",
    "## Dev Recommendation\n",
    "\n",
    "It's highly recommended that you run this notebook inside a virtual environment. This works best if you have Anaconda, as you can then specify the Python version:\n",
    "\n",
    "    # Create and activate environment\n",
    "    conda create -n hotel-reviews python=3.11 -y\n",
    "    conda activate hotel-reviews\n",
    "\n",
    "    # Install PyTorch via Conda for handling of CUDA\n",
    "    conda install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "\n",
    "    # Install and register the Jupyter Notebook kernel inside your env\n",
    "    pip install ipykernel\n",
    "    python -m ipykernel install --user --name hotel-reviews --display-name \"Hotel Reviews (3.11)\"\n",
    "\n",
    "You should then be able to select the newly installed kernel as your Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad697da-ca50-4579-b199-dc3672c6dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install transformers datasets peft trl bitsandbytes accelerate tqdm\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67728679-ac75-47b1-88d4-7a63dd9de140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df01d2-e0f3-438a-913a-64163577a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Test bitsandbytes specifically — this is the one that breaks most often\n",
    "import bitsandbytes as bnb\n",
    "print(f\"bitsandbytes: {bnb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b9492-3b22-417b-a49c-8bddf2224af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import json\n",
    "with open(\"preprocessed_reviews.json\",\"r\") as f:\n",
    "    dataset = [json.loads(s) for s in f.readlines()]\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518607ef-a2f1-4bc1-94cf-c5b5812a4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/test dataset\n",
    "import random\n",
    "\n",
    "TRAIN_SIZE = 50000\n",
    "VAL_SIZE = 2000\n",
    "\n",
    "SAMPLE_TOTAL = TRAIN_SIZE + VAL_SIZE\n",
    "dataset_sample = random.sample(dataset, SAMPLE_TOTAL)\n",
    "\n",
    "dataset_train = dataset_sample[:TRAIN_SIZE]\n",
    "dataset_val = dataset_sample[TRAIN_SIZE:]\n",
    "\n",
    "print(f\"Train: {len(dataset_train)} records; Val: {len(dataset_val)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade3012-d3b9-4f89-bb17-4c6772813613",
   "metadata": {},
   "source": [
    "In this initial version, we will focus only on a few features:\n",
    "\n",
    "- Rating\n",
    "- Graded since-stay time (0 through 3)\n",
    "\n",
    "We will expand with amenities and other functionality after we test with this initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff57ef7-6c74-4e33-b6ed-eb17fc96beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset by sampling the available data\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def create_sample(data_record):\n",
    "\n",
    "    this_rating = round(data_record['score'])\n",
    "    stay_latency = data_record['review_elapsed']\n",
    "    user_request = {\n",
    "        \"rating\": this_rating,\n",
    "        \"days_since_stay\": stay_latency\n",
    "    }\n",
    "    ai_response = f\"Positive Review: {data_record['positive']}\\n\\nNegative Review: {data_record['negative']}\"\n",
    "\n",
    "    final_record = {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"user\", \"content\": json.dumps(user_request)},\n",
    "            {\"role\": \"assistant\", \"content\": ai_response}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return json.dumps(final_record)\n",
    "\n",
    "dataset_train_file = [create_sample(x) for x in tqdm.tqdm(dataset_train,desc=\"Prepare train dataset\")]\n",
    "dataset_val_file = [create_sample(x) for x in tqdm.tqdm(dataset_val,desc=\"Prepare validation dataset\")]\n",
    "\n",
    "open(\"train.jsonl\",\"w\").write(\"\\n\".join(dataset_train_file))\n",
    "open(\"val.jsonl\",\"w\").write(\"\\n\".join(dataset_val_file))\n",
    "\n",
    "print(\"Datasets prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3bd85-583d-4eca-a19e-cd31c75a1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples from the dataset\n",
    "from IPython.display import JSON\n",
    "\n",
    "JSON(dataset_train_file[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad64d6-269f-4e3a-a391-ff5c358670c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load base model in 4-bit\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import os\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "# Also check a specific parameter\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is {param.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4727f46-4ec8-4ade-b783-e9aff0941326",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"eval\": \"val.jsonl\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413a1fc-8d80-4e5d-9547-cec97eca9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 4. Configure training\n",
    "training_config = SFTConfig(\n",
    "    output_dir=\"./hotel-review-lora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    #max_seq_length=768,\n",
    "    dataset_text_field=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb78af-9442-48d0-8391-669c0f4e8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset as required\n",
    "def format_chat(example):\n",
    "    messages = example[\"conversations\"]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_chat, remove_columns=[\"conversations\"])\n",
    "\n",
    "print(\"Dataset preparation completed.\")\n",
    "\n",
    "# Verify it looks right\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5a0e0-ef75-4e86-99b3-eae49db2779f",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b7566-bdf5-48f4-85e5-9ab578d9ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create trainer and run\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"eval\"],\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80be7ec-dd26-479e-8f50-87f23ad23237",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./hotel-review-lora\")\n",
    "tokenizer.save_pretrained(\"./hotel-review-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f59d7-4a71-418d-8c8b-7cfc3e1c3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training is complete!\n",
    "# Clear the environment\n",
    "\n",
    "del trainer\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c7288-100e-4b76-8de8-8fb8317ea44a",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afd8eb-3712-40b7-a2ce-d216edb80096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new model\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Load base model in 4-bit (same as training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically places layers on available GPUs\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "# Apply your LoRA adapter on top\n",
    "model = PeftModel.from_pretrained(base_model, \"./hotel-review-lora\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800bf39-e28b-4f10-94bf-c2bcfa27864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rating: int, days_since_stay: int):\n",
    "    # Generate\n",
    "    \n",
    "    prompt = {\"rating\": rating, \"days_since_stay\": days_since_stay}\n",
    "    prompt = json.dumps(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    review = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return review\n",
    "\n",
    "for i in tqdm.trange(10):\n",
    "    print(generate(i+1, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9b6051-3b44-4875-ab0f-f95e376a217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 291/291 [00:02<00:00, 126.23it/s, Materializing param=model.norm.weight]                              \n",
      "Writing model shards: 100%|██████████| 1/1 [01:42<00:00, 102.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Mistral-HotelReviews-7b/tokenizer_config.json',\n",
       " './Mistral-HotelReviews-7b/chat_template.jinja',\n",
       " './Mistral-HotelReviews-7b/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the model into a single completed model prior to GGUF conversion\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load base model at full precision for merging\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",  # Merge on CPU to avoid VRAM issues\n",
    ")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./hotel-review-lora\")\n",
    "\n",
    "# Merge weights\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save\n",
    "merged_model.save_pretrained(\"./Mistral-HotelReviews-7b\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.save_pretrained(\"./Mistral-HotelReviews-7b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
